#!/usr/bin/env python2
# -*- coding: utf-8 -*-
from __future__ import print_function

import itertools as it, operator as op, functools as ft
from collections import OrderedDict
from contextlib import contextmanager, closing
from os.path import exists, dirname
import subprocess, tempfile, time, glob, socket
import os, sys, re, json, types, base64

import requests


def get_uid(n=3):
	assert n * 8 % 6 == 0, n
	return base64.urlsafe_b64encode(os.urandom(n))

def log_lines(log_func, lines, log_func_last=False):
	'''Log passed sequence of lines or a newline-delimited string via log_func.
		Sequence elements can also be tuples of (template, *args, **kws) to pass to log_func.
		log_func_last (if passed) will be applied to the last line instead of log_func,
			with idea behind it is to pass e.g. log.exception there,
			so it'd dump proper traceback at the end of the message.'''
	if isinstance(lines, types.StringTypes):
		lines = list(line.rstrip() for line in lines.rstrip().split('\n'))
	uid = get_uid()
	for n, line in enumerate(lines, 1):
		if isinstance(line, types.StringTypes): line = '[%s] %s', uid, line
		else: line = ('[{}] {}'.format(uid, line[0]),) + line[1:]
		if log_func_last and n == len(lines): log_func_last(*line)
		else: log_func(*line)

def parse_pos_spec(pos):
	try: mins, secs = pos.rsplit(':', 1)
	except ValueError: hrs, mins, secs = 0, 0, pos
	else:
		try: hrs, mins = mins.rsplit(':', 1)
		except ValueError: hrs = 0
	return sum( a*b for a, b in
		it.izip([3600, 60, 1], map(float, [hrs, mins, secs])) )


class VodFileCache(object):

	update_lock = True

	def __init__(self, prefix, ext):
		self.path = '{}.{}'.format(prefix, ext)

	@property
	def cached(self):
		if not exists(self.path): return None
		with open(self.path, 'rb') as src: return src.read()

	def update(self, data):
		assert not self.update_lock
		with open(self.path, 'wb') as dst: dst.write(data)
		return data

	def __enter__(self):
		assert self.update_lock
		self.update_lock = False
		return self

	def __exit__(self, *err):
		self.update_lock = True


@contextmanager
def req(method, *args, **kws):
	if not getattr(req, 's', None):
		# Mostly for cases when aria2c lags on startup
		from requests.packages.urllib3.util.retry import Retry
		req.s = requests.Session()
		req.s.mount( 'http://',
			requests.adapters.HTTPAdapter(
				max_retries=Retry(total=4, backoff_factor=1) ) )

	with closing(req.s.request(method, *args, **kws)) as r:
		try: r.raise_for_status()
		except Exception as err:
			log_lines(log.error, [
				'HTTP request failed:',
				('  args: %s', args), ('  kws: %s', kws),
				('  response content: %s', r.content) ])
			raise
		yield r

req_jrpc_uid = lambda _ns=get_uid(),\
	_seq=iter(xrange(1, 2**30)): '{}.{}'.format(_ns, next(_seq))

def req_jrpc(url, method, *params):
	data_req = dict(
		jsonrpc='2.0', id=req_jrpc_uid(),
		method=method, params=params )
	with req('post', url, json=data_req) as r: data_res = r.json()
	assert data_res.get('result') is not None, [data_req, data_res]
	return data_res['result']


def main(args=None):
	import argparse
	parser = argparse.ArgumentParser(
		usage='%(prog)s [options] url file_prefix',
		description='Grab a VoD or a specified slice of it from twitch.tv, properly.')
	parser.add_argument('url', help='URL for a VoD to fetch.')
	parser.add_argument('file_prefix', help='File prefix to assemble temp files under.')

	parser.add_argument('-y', '--ytdl-opts',
		action='append', metavar='opts',
		help='Extra opts for youtube-dl --get-url command.'
			' Will be split on spaces, unless option is used multiple times.')
	parser.add_argument('-a', '--aria2c-opts',
		action='append', metavar='opts',
		help='Extra opts for aria2c command.'
			' Will be split on spaces, unless option is used multiple times.')

	parser.add_argument('-s', '--start-pos',
		metavar='[[hours:]minutes:]seconds',
		help='Only download video chunks after specified start position.')
	parser.add_argument('-l', '--length',
		metavar='[[hours:]minutes:]seconds',
		help='Only download specified length of the video (from specified start or beginning).')

	parser.add_argument('-k', '--keep-tempfiles',
		action='store_true', help='Do not remove all the'
				' temporary files after successfully assembling resulting mp4.'
			' Chunks in particular might be useful to download different but overlapping video slices.')

	parser.add_argument('--debug', action='store_true', help='Verbose operation mode.')
	opts = parser.parse_args(sys.argv[1:] if args is None else args)

	global log
	import logging
	logging.basicConfig(
		datefmt='%Y-%m-%d %H:%M:%S',
		format='%(asctime)s :: %(name)s %(levelname)s :: %(message)s',
		level=logging.DEBUG if opts.debug else logging.INFO )
	log = logging.getLogger('main')

	# Retries logged from here are kinda useless, especially before aria2c starts
	logging.getLogger('requests.packages.urllib3.connectionpool').setLevel(logging.ERROR)

	if re.search(r'^https?:', opts.file_prefix):
		if re.search(r'^https?:', opts.url):
			parser.error('Both args seem to be an URL, only first one should be.')
		log.warn('Looks like url/prefix args got mixed-up, correcting that')
		opts.file_prefix, opts.url = opts.url, opts.file_prefix

	start_delay = parse_pos_spec(opts.start_pos) if opts.start_pos else 0
	max_length = opts.length and parse_pos_spec(opts.length)

	vod_cache = ft.partial(VodFileCache, opts.file_prefix)

	with vod_cache('m3u8.url') as vc:
		url_pls = vc.cached
		if not url_pls:
			ytdl_opts = opts.ytdl_opts or list()
			if len(ytdl_opts) == 1: ytdl_opts = ytdl_opts[0].split()
			cmd = ['youtube-dl', '--get-url'] + ytdl_opts + [opts.url]
			log.debug('Running "youtube-dl --get-url" command: %s', ' '.join(cmd))
			url_pls = vc.update(subprocess.check_output(cmd, close_fds=True).strip())
	url_base = url_pls.rsplit('/', 1)[0]

	with vod_cache('m3u8.ua') as vc:
		ua = vc.cached
		if not ua:
			ua = vc.update(subprocess.check_output(
				['youtube-dl', '--dump-user-agent'], close_fds=True ).strip())

	with vod_cache('m3u8') as vc:
		pls = vc.cached
		if not pls:
			with req('get', url_pls, headers={'user-agent': ua}) as r:
				pls = vc.update(r.text)

	# port/key are always updated between aria2c runs
	with vod_cache('rpc_key') as vc: key = vc.update(get_uid(18))
	with vod_cache('rpc_port') as vc:
		with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
			s.bind(('localhost', 0))
			addr, port = s.getsockname()
		port = int(vc.update(bytes(port)))

	# aria2c requires 16-char gid, gid_format fits number in
	#  first 6 chars because it looks nice in (tuncated) aria2c output
	gid_seq = iter(xrange(1, 2**30))
	gid_format, gid_parse = '{:06d}0000000000'.format, lambda gid_str: int(gid_str[:6])
	with vod_cache('gids') as vc:
		gids_done, gids_done_path = vc.cached, vc.path
		if not gids_done: gids_done = vc.update('')
		gids_done = set(it.imap(gid_parse, gids_done.splitlines()))
	gids_started, gids_needed = OrderedDict(), list()

	# Try to pick leaner shell than bash, since hook is dead-simple
	# sh hook(s) assume that gids_*_path fs supports atomic O_APPEND
	sh_path = '/bin/dash'
	if not exists(sh_path): sh_path = '/bin/ash' # busybox
	if not exists(sh_path): sh_path = '/bin/sh'

	hook_done = '/tmp/.fetch_twitch_vod.{}.done.hook'.format(os.getpid())
	assert "'" not in opts.file_prefix, opts.file_prefix
	assert "'" not in gids_done_path, gids_done_path
	with open(hook_done, 'wb') as dst:
		dst.write('\n'.join([
			'#!{}'.format(sh_path),
			'mv "$3" \'{}\'.chunk."$1".mp4\\'.format(opts.file_prefix),
			'  && echo "$1" >>\'{}\''.format(gids_done_path) ]))
		os.fchmod(dst.fileno(), 0700)

	aria2c_log_level = 'notice' if opts.debug else 'warn'
	aria2c_opts = opts.aria2c_opts or list()
	if len(aria2c_opts) == 1: aria2c_opts = aria2c_opts[0].split()
	aria2c = subprocess.Popen([
		'aria2c',

		'--summary-interval=0',
		'--console-log-level={}'.format(aria2c_log_level),

		'--stop-with-process={}'.format(os.getpid()),
		'--enable-rpc=true',
		'--rpc-listen-port={}'.format(port),
		'--rpc-secret={}'.format(key),

		'--no-netrc', '--no-proxy',
		'--always-resume=false',
		'--max-concurrent-downloads=5',
		'--max-connection-per-server=5',
		'--max-file-not-found=5',
		'--max-tries=8',
		'--timeout=15',
		'--connect-timeout=10',
		'--lowest-speed-limit=100K',
		'--user-agent={}'.format(ua),

		'--on-download-complete={}'.format(hook_done),
	] + aria2c_opts, close_fds=True)
	aria2c_exit_clean = False
	aria2c_jrpc = ft.partial(req_jrpc, 'http://localhost:{}/jsonrpc'.format(port))

	log.debug('Starting downloads (rpc port: %s)...', port)
	try:
		key = 'token:{}'.format(key)
		line_buff, line_buff_max = list(), 50
		wait_last_gids, poll_delay = 100, 5
		chunk_err_retries, chunk_err_retry_delay = 10, 2

		def queue_gid_downloads(gid_files):
			# system.multicall(methods)
			# aria2.addUri([secret], uris[, options[, position]])
			res = aria2c_jrpc('system.multicall', list(
				dict( methodName='aria2.addUri',
					params=[ key, ['{}/{}'.format(url_base, path)],
						dict(gid=gid_str, out='{}.chunk.{}.mp4.tmp'.format(opts.file_prefix, gid_str)) ] )
				for gid_str, path in gid_files ))
			res_chk = list([gid] for gid, path in gid_files)
			if res != res_chk:
				log_lines(log.error, [
					'Result gid match check failed for submitted urls.',
					('  expected: %s', ', '.join(bytes(r[0]) for r in res_chk)),
					('  returned: %s', ', '.join(( bytes(r[0])
						if isinstance(r, list) else repr(r) ) for r in res)) ])
				raise RuntimeError('Result gid match check failed')

		def line_buff_flush():
			gid_files = list((next(gid_seq), path) for path in line_buff)
			gids_needed.extend(map(op.itemgetter(0), gid_files))
			gid_files = list((gid, path) for gid, path in gid_files if gid not in gids_done)
			gids_started.update(gid_files)
			gid_files = list((gid_format(gid), path) for gid, path in gid_files)
			if gid_files: queue_gid_downloads(gid_files)
			del line_buff[:]

		### Queue all initial downloads
		for line in pls.splitlines():
			m = re.search(r'^#EXTINF:([\d.]+),', line)
			if m: start_delay -= float(m.group(1))
			if start_delay > 0: continue
			if not line or line.startswith('#'): continue
			if max_length and max_length + start_delay < 0: break
			line_buff.append(line)
			if len(line_buff) >= line_buff_max: line_buff_flush()
		if line_buff: line_buff_flush()

		### Wait-to-complete - check missing - retry loop
		gid_retries, gids_started_count = None, len(gids_started)
		gid_last = list(gids_started)[-1] if gids_started else 0
		log.info( '\n\n  ------ Started %s downloads,'
			' last gid: %06d ------  \n', gids_started_count, gid_last )
		for n in xrange(1, chunk_err_retries+1):
			if gid_retries:
				aria2c_jrpc('aria2.purgeDownloadResult', key)
				queue_gid_downloads(gid_retries)

			while True:
				gids_wait = len(aria2c_jrpc('aria2.tellActive', key, ['status']))
				res = aria2c_jrpc('aria2.tellWaiting', key, 0, wait_last_gids, ['status'])
				if len(res) == wait_last_gids: gids_wait = '>{}'.format(wait_last_gids)
				else: gids_wait += len(res)
				if not gids_wait: break
				log.debug( # helps to see the overall progress
					'\n\n  ------ waiting for downloads (count: %s'
						' / %s, last gid: %06d, err-retry-pass: %s) ------  \n',
					gids_wait, gids_started_count, gid_last, n )
				time.sleep(poll_delay)

			with vod_cache('gids') as vc:
				for gid in it.imap(gid_parse, vc.cached.splitlines()): gids_started.pop(gid, None)
			gid_retries = list((gid_format(gid), path) for gid, path in gids_started.viewitems())
			if not gid_retries: break
			gid_last = list(gids_started)[-1]
			log.debug('Chunk retry delay (failed: %s): %ss', len(gid_retries), chunk_err_retry_delay)
			time.sleep(chunk_err_retry_delay)
		else:
			log.error('Failed to download %s chunks (after %s retries)', len(gid_retries), n)

		### Proper shutdown
		aria2c_jrpc('aria2.shutdown', key)
		aria2c_exit_clean = not gid_retries
		log.debug(
			'Finished with downloads (%s chunks downloaded, %s failed, %s existing)',
			gids_started_count - len(gid_retries), len(gid_retries), len(gids_done) )

	finally:
		if not aria2c_exit_clean: aria2c.terminate()
		aria2c.wait()
		os.unlink(hook_done)

	if not aria2c_exit_clean:
		log.error('Unresolved download errors detected, aborting')
		return 1

	chunks_needed = list(
		'{}.chunk.{}.mp4'.format(opts.file_prefix, gid_format(gid))
		for gid in gids_needed )
	chunks = filter(exists, chunks_needed)
	chunks_missing = set(chunks_needed).difference(chunks)
	if chunks_missing:
		log.error(
			'Aborting due to %s missing chunk(s)'
				' (use --debug for full list, fix/remove %r to re-download)',
			len(chunks_missing), gids_done_path )
		log_lines( log.debug, ['Missing chunks:']
			+ list(('  %s', chunk) for chunk in sorted(chunks_missing)) )
		return 1
	assert sorted(chunks) == chunks

	log.info('Concatenating %s chunk files...', len(chunks))
	dst_file = '{}.mp4'.format(opts.file_prefix)

	## Simple "cat" works for players like vlc and mpv, with minor seek inprecision
	if not exists(dst_file):
		dst_file_tmp = '{}.tmp'.format(dst_file)
		with open(dst_file_tmp, 'wb') as dst:
			subprocess.check_call(['cat'] + chunks, stdout=dst, close_fds=True)
		os.rename(dst_file_tmp, dst_file)

	## ffmpeg otoh makes file unplayable on the second pass, well done!
	# with vod_cache('chunk.ffmpeg_concat') as vc:
	# 	pls_chunks_path = vc.path
	# 	if not vc.cached: vc.update(''.join(map('file \'{}\'\n'.format, chunks)))
	# if not exists(dst_file):
	# 	subprocess.check_call([ 'ffmpeg',
	# 		'-y', '-f', 'concat', '-i', pls_chunks_path,
	# 		'-movflags', 'empty_moov', # makes file playable right away
	# 		'-c', 'copy', '-bsf:a', 'aac_adtstoasc', dst_file ])

	if not opts.keep_tempfiles:
		tmp_files = list(it.chain(( vod_cache(ext).path for ext in
				['m3u8.url', 'm3u8.ua', 'm3u8', 'rpc_key', 'rpc_port', 'gids'] ), chunks))
		log.debug('Cleaning up temporary files (count: %s)...', len(tmp_files))
		for p in tmp_files: os.unlink(p)

	log.info('Finished, resulting file: %s', dst_file)

if __name__ == '__main__': sys.exit(main())
